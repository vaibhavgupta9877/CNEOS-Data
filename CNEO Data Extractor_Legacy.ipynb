{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://VAibhAv:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CNEO_data_extractor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e64aa3e440>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('CNEO_data_extractor').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-03 2023-03-04\n"
     ]
    }
   ],
   "source": [
    "date = datetime.today().date()\n",
    "date_min = date + timedelta(days=59)\n",
    "date_max = date + timedelta(days=60)\n",
    "print(date_min, date_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://ssd-api.jpl.nasa.gov/cad.api\"\n",
    "# params = {\n",
    "#     \"date-min\": str(date),\n",
    "#     \"dist-max\": \"0.1\",\n",
    "#     'fullname': \"true\",\n",
    "#     'dist-max': \"0.1\",\n",
    "#     'diameter': \"true\"\n",
    "# }\n",
    "# response2 = requests.get(url, params)\n",
    "# data2 = response2.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ssd-api.jpl.nasa.gov/cad.api\"\n",
    "parameters = {\n",
    "    \"date-min\": \"1900-01-04\",\n",
    "    \"date-max\": str(datetime.today().date()),\n",
    "    \"dist-max\": \"0.05\",\n",
    "    'fullname': \"true\",\n",
    "    'dist-max': \"0.1\",\n",
    "    'diameter': \"true\"\n",
    "}\n",
    "response = requests.get(url, parameters)\n",
    "data = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Designation',\n",
    "    'Orbit Id',\n",
    "    'Time of Close approach',\n",
    "    'Close-Approach Date',\n",
    "    'Nominal Approch distance (au)',\n",
    "    'Min Close-Approach Distance (au)',\n",
    "    'Max Close-Approach Distance (au)',\n",
    "    'V Reletive (Km/s)',\n",
    "    'V Infinite (Km/s)',\n",
    "    'Close-Approach Uncertain Time',\n",
    "    'Absolute Magnitude (mag)',\n",
    "    'Diameter (Km)',\n",
    "    'Diameter-Sigma (Km)',\n",
    "    'Object'\n",
    "]\n",
    "# types = [\n",
    "#     StringType(),\n",
    "#     StringType(),\n",
    "#     DoubleType(),\n",
    "#     DateType(),\n",
    "#     DoubleType(),\n",
    "#     DoubleType(),\n",
    "#     DoubleType(),\n",
    "#     DoubleType(),\n",
    "#     DoubleType(),\n",
    "#     StringType(),\n",
    "#     DoubleType(),\n",
    "#     DoubleType(),\n",
    "#     DoubleType(),\n",
    "#     StringType(),\n",
    "#          ]\n",
    "# dic = {columns[i]: types[i] for i in range(len(columns))}\n",
    "# dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>des</th>\n",
       "      <th>orbit_id</th>\n",
       "      <th>jd</th>\n",
       "      <th>cd</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_min</th>\n",
       "      <th>dist_max</th>\n",
       "      <th>v_rel</th>\n",
       "      <th>v_inf</th>\n",
       "      <th>t_sigma_f</th>\n",
       "      <th>h</th>\n",
       "      <th>diameter</th>\n",
       "      <th>diameter_sigma</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 BN7</td>\n",
       "      <td>6</td>\n",
       "      <td>2415023.594589649</td>\n",
       "      <td>1900-Jan-04 02:16</td>\n",
       "      <td>0.0896607474147164</td>\n",
       "      <td>0.0882582365913522</td>\n",
       "      <td>0.0914306781836958</td>\n",
       "      <td>5.2581158476114</td>\n",
       "      <td>5.25246109718832</td>\n",
       "      <td>1_13:09</td>\n",
       "      <td>23.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2020 BN7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017 MW4</td>\n",
       "      <td>18</td>\n",
       "      <td>2415023.595882106</td>\n",
       "      <td>1900-Jan-04 02:18</td>\n",
       "      <td>0.0613004997707699</td>\n",
       "      <td>0.0612907488305461</td>\n",
       "      <td>0.0613102521172838</td>\n",
       "      <td>17.5916418448861</td>\n",
       "      <td>17.5891708464552</td>\n",
       "      <td>00:01</td>\n",
       "      <td>20.05</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2017 MW4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>509352</td>\n",
       "      <td>57</td>\n",
       "      <td>2415024.433789572</td>\n",
       "      <td>1900-Jan-04 22:25</td>\n",
       "      <td>0.00963183861698794</td>\n",
       "      <td>0.00962494789186613</td>\n",
       "      <td>0.00963873093215005</td>\n",
       "      <td>8.68671104177175</td>\n",
       "      <td>8.65480697513416</td>\n",
       "      <td>00:02</td>\n",
       "      <td>20.16</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>509352 (2007 AG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214869</td>\n",
       "      <td>173</td>\n",
       "      <td>2415026.944065756</td>\n",
       "      <td>1900-Jan-07 10:39</td>\n",
       "      <td>0.0501688442324355</td>\n",
       "      <td>0.0501602845857579</td>\n",
       "      <td>0.0501774043027291</td>\n",
       "      <td>11.5943349372234</td>\n",
       "      <td>11.5897533259902</td>\n",
       "      <td>00:02</td>\n",
       "      <td>16.51</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>214869 (2007 PA8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017 QD3</td>\n",
       "      <td>7</td>\n",
       "      <td>2415029.500845714</td>\n",
       "      <td>1900-Jan-10 00:01</td>\n",
       "      <td>0.0865599287526722</td>\n",
       "      <td>0.08654753449196</td>\n",
       "      <td>0.0865723230832191</td>\n",
       "      <td>10.9317273117942</td>\n",
       "      <td>10.928911117456</td>\n",
       "      <td>00:03</td>\n",
       "      <td>22.23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2017 QD3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        des orbit_id                 jd                 cd  \\\n",
       "0  2020 BN7        6  2415023.594589649  1900-Jan-04 02:16   \n",
       "1  2017 MW4       18  2415023.595882106  1900-Jan-04 02:18   \n",
       "2    509352       57  2415024.433789572  1900-Jan-04 22:25   \n",
       "3    214869      173  2415026.944065756  1900-Jan-07 10:39   \n",
       "4  2017 QD3        7  2415029.500845714  1900-Jan-10 00:01   \n",
       "\n",
       "                  dist             dist_min             dist_max  \\\n",
       "0   0.0896607474147164   0.0882582365913522   0.0914306781836958   \n",
       "1   0.0613004997707699   0.0612907488305461   0.0613102521172838   \n",
       "2  0.00963183861698794  0.00962494789186613  0.00963873093215005   \n",
       "3   0.0501688442324355   0.0501602845857579   0.0501774043027291   \n",
       "4   0.0865599287526722     0.08654753449196   0.0865723230832191   \n",
       "\n",
       "              v_rel             v_inf t_sigma_f      h diameter  \\\n",
       "0   5.2581158476114  5.25246109718832   1_13:09   23.8     None   \n",
       "1  17.5916418448861  17.5891708464552     00:01  20.05     None   \n",
       "2  8.68671104177175  8.65480697513416     00:02  20.16     None   \n",
       "3  11.5943349372234  11.5897533259902     00:02  16.51     None   \n",
       "4  10.9317273117942   10.928911117456     00:03  22.23     None   \n",
       "\n",
       "  diameter_sigma           fullname  \n",
       "0           None         (2020 BN7)  \n",
       "1           None         (2017 MW4)  \n",
       "2           None   509352 (2007 AG)  \n",
       "3           None  214869 (2007 PA8)  \n",
       "4           None         (2017 QD3)  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array(data['data']), columns = data['fields'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>des</th>\n",
       "      <th>orbit_id</th>\n",
       "      <th>jd</th>\n",
       "      <th>cd</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_min</th>\n",
       "      <th>dist_max</th>\n",
       "      <th>v_rel</th>\n",
       "      <th>v_inf</th>\n",
       "      <th>t_sigma_f</th>\n",
       "      <th>h</th>\n",
       "      <th>diameter</th>\n",
       "      <th>diameter_sigma</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46777</th>\n",
       "      <td>2022 YU3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 05:11:00</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.025017</td>\n",
       "      <td>0.025365</td>\n",
       "      <td>7.133786</td>\n",
       "      <td>7.118944</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YU3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46778</th>\n",
       "      <td>2022 YZ3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 09:29:00</td>\n",
       "      <td>0.060599</td>\n",
       "      <td>0.060118</td>\n",
       "      <td>0.061080</td>\n",
       "      <td>11.704659</td>\n",
       "      <td>11.700902</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YZ3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46779</th>\n",
       "      <td>2022 YY6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 11:07:00</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>20.269551</td>\n",
       "      <td>20.245368</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>26.109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YY6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46780</th>\n",
       "      <td>2022 YP5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 17:53:00</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>4.793711</td>\n",
       "      <td>4.765807</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>27.085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YP5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46781</th>\n",
       "      <td>2021 NF</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 21:51:00</td>\n",
       "      <td>0.045947</td>\n",
       "      <td>0.039555</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>11.326777</td>\n",
       "      <td>11.321656</td>\n",
       "      <td>03:09</td>\n",
       "      <td>24.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2021 NF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            des orbit_id            jd                  cd      dist  \\\n",
       "46777  2022 YU3        5  2.459947e+06 2023-01-02 05:11:00  0.025191   \n",
       "46778  2022 YZ3        3  2.459947e+06 2023-01-02 09:29:00  0.060599   \n",
       "46779  2022 YY6        4  2.459947e+06 2023-01-02 11:07:00  0.005439   \n",
       "46780  2022 YP5        6  2.459947e+06 2023-01-02 17:53:00  0.019977   \n",
       "46781   2021 NF        6  2.459947e+06 2023-01-02 21:51:00  0.045947   \n",
       "\n",
       "       dist_min  dist_max      v_rel      v_inf t_sigma_f       h  diameter  \\\n",
       "46777  0.025017  0.025365   7.133786   7.118944   < 00:01  25.835       NaN   \n",
       "46778  0.060118  0.061080  11.704659  11.700902   < 00:01  25.088       NaN   \n",
       "46779  0.005411  0.005467  20.269551  20.245368   < 00:01  26.109       NaN   \n",
       "46780  0.019919  0.020036   4.793711   4.765807   < 00:01  27.085       NaN   \n",
       "46781  0.039555  0.052455  11.326777  11.321656     03:09  24.730       NaN   \n",
       "\n",
       "       diameter_sigma           fullname  \n",
       "46777             NaN         (2022 YU3)  \n",
       "46778             NaN         (2022 YZ3)  \n",
       "46779             NaN         (2022 YY6)  \n",
       "46780             NaN         (2022 YP5)  \n",
       "46781             NaN          (2021 NF)  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['jd'] = pd.to_numeric(df['jd'])\n",
    "df['cd'] = pd.to_datetime(df['cd'])\n",
    "df['dist'] = pd.to_numeric(df['dist'])\n",
    "df['dist_min'] = pd.to_numeric(df['dist_min'])\n",
    "df['dist_max'] = pd.to_numeric(df['dist_max'])\n",
    "df['v_rel'] = pd.to_numeric(df['v_rel'])\n",
    "df['v_inf'] = pd.to_numeric(df['v_inf'])\n",
    "df['t_sigma_f'] = df['t_sigma_f'].astype(str)\n",
    "df['h'] = pd.to_numeric(df['h'])\n",
    "df['diameter'] = pd.to_numeric(df['diameter'])\n",
    "df['diameter_sigma'] = pd.to_numeric(df['diameter_sigma'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>des</th>\n",
       "      <th>orbit_id</th>\n",
       "      <th>jd</th>\n",
       "      <th>cd</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_min</th>\n",
       "      <th>dist_max</th>\n",
       "      <th>v_rel</th>\n",
       "      <th>v_inf</th>\n",
       "      <th>t_sigma_f</th>\n",
       "      <th>h</th>\n",
       "      <th>diameter</th>\n",
       "      <th>diameter_sigma</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 BN7</td>\n",
       "      <td>6</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 02:16:00</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.088258</td>\n",
       "      <td>0.091431</td>\n",
       "      <td>5.258116</td>\n",
       "      <td>5.252461</td>\n",
       "      <td>1_13:09</td>\n",
       "      <td>23.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2020 BN7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017 MW4</td>\n",
       "      <td>18</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 02:18:00</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.061291</td>\n",
       "      <td>0.061310</td>\n",
       "      <td>17.591642</td>\n",
       "      <td>17.589171</td>\n",
       "      <td>00:01</td>\n",
       "      <td>20.050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2017 MW4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>509352</td>\n",
       "      <td>57</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 22:25:00</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.009639</td>\n",
       "      <td>8.686711</td>\n",
       "      <td>8.654807</td>\n",
       "      <td>00:02</td>\n",
       "      <td>20.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>509352 (2007 AG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214869</td>\n",
       "      <td>173</td>\n",
       "      <td>2.415027e+06</td>\n",
       "      <td>1900-01-07 10:39:00</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.050160</td>\n",
       "      <td>0.050177</td>\n",
       "      <td>11.594335</td>\n",
       "      <td>11.589753</td>\n",
       "      <td>00:02</td>\n",
       "      <td>16.510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214869 (2007 PA8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017 QD3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.415030e+06</td>\n",
       "      <td>1900-01-10 00:01:00</td>\n",
       "      <td>0.086560</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.086572</td>\n",
       "      <td>10.931727</td>\n",
       "      <td>10.928911</td>\n",
       "      <td>00:03</td>\n",
       "      <td>22.230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2017 QD3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46777</th>\n",
       "      <td>2022 YU3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 05:11:00</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.025017</td>\n",
       "      <td>0.025365</td>\n",
       "      <td>7.133786</td>\n",
       "      <td>7.118944</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YU3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46778</th>\n",
       "      <td>2022 YZ3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 09:29:00</td>\n",
       "      <td>0.060599</td>\n",
       "      <td>0.060118</td>\n",
       "      <td>0.061080</td>\n",
       "      <td>11.704659</td>\n",
       "      <td>11.700902</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YZ3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46779</th>\n",
       "      <td>2022 YY6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 11:07:00</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>20.269551</td>\n",
       "      <td>20.245368</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>26.109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YY6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46780</th>\n",
       "      <td>2022 YP5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 17:53:00</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>4.793711</td>\n",
       "      <td>4.765807</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>27.085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YP5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46781</th>\n",
       "      <td>2021 NF</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 21:51:00</td>\n",
       "      <td>0.045947</td>\n",
       "      <td>0.039555</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>11.326777</td>\n",
       "      <td>11.321656</td>\n",
       "      <td>03:09</td>\n",
       "      <td>24.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2021 NF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46782 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            des orbit_id            jd                   cd      dist  \\\n",
       "0      2020 BN7        6  2.415024e+06  1900-01-04 02:16:00  0.089661   \n",
       "1      2017 MW4       18  2.415024e+06  1900-01-04 02:18:00  0.061300   \n",
       "2        509352       57  2.415024e+06  1900-01-04 22:25:00  0.009632   \n",
       "3        214869      173  2.415027e+06  1900-01-07 10:39:00  0.050169   \n",
       "4      2017 QD3        7  2.415030e+06  1900-01-10 00:01:00  0.086560   \n",
       "...         ...      ...           ...                  ...       ...   \n",
       "46777  2022 YU3        5  2.459947e+06  2023-01-02 05:11:00  0.025191   \n",
       "46778  2022 YZ3        3  2.459947e+06  2023-01-02 09:29:00  0.060599   \n",
       "46779  2022 YY6        4  2.459947e+06  2023-01-02 11:07:00  0.005439   \n",
       "46780  2022 YP5        6  2.459947e+06  2023-01-02 17:53:00  0.019977   \n",
       "46781   2021 NF        6  2.459947e+06  2023-01-02 21:51:00  0.045947   \n",
       "\n",
       "       dist_min  dist_max      v_rel      v_inf t_sigma_f       h  diameter  \\\n",
       "0      0.088258  0.091431   5.258116   5.252461   1_13:09  23.800       NaN   \n",
       "1      0.061291  0.061310  17.591642  17.589171     00:01  20.050       NaN   \n",
       "2      0.009625  0.009639   8.686711   8.654807     00:02  20.160       NaN   \n",
       "3      0.050160  0.050177  11.594335  11.589753     00:02  16.510       NaN   \n",
       "4      0.086548  0.086572  10.931727  10.928911     00:03  22.230       NaN   \n",
       "...         ...       ...        ...        ...       ...     ...       ...   \n",
       "46777  0.025017  0.025365   7.133786   7.118944   < 00:01  25.835       NaN   \n",
       "46778  0.060118  0.061080  11.704659  11.700902   < 00:01  25.088       NaN   \n",
       "46779  0.005411  0.005467  20.269551  20.245368   < 00:01  26.109       NaN   \n",
       "46780  0.019919  0.020036   4.793711   4.765807   < 00:01  27.085       NaN   \n",
       "46781  0.039555  0.052455  11.326777  11.321656     03:09  24.730       NaN   \n",
       "\n",
       "       diameter_sigma           fullname  \n",
       "0                 NaN         (2020 BN7)  \n",
       "1                 NaN         (2017 MW4)  \n",
       "2                 NaN   509352 (2007 AG)  \n",
       "3                 NaN  214869 (2007 PA8)  \n",
       "4                 NaN         (2017 QD3)  \n",
       "...               ...                ...  \n",
       "46777             NaN         (2022 YU3)  \n",
       "46778             NaN         (2022 YZ3)  \n",
       "46779             NaN         (2022 YY6)  \n",
       "46780             NaN         (2022 YP5)  \n",
       "46781             NaN          (2021 NF)  \n",
       "\n",
       "[46782 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./raw_input_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Designation</th>\n",
       "      <th>Orbit Id</th>\n",
       "      <th>Time of Close approach</th>\n",
       "      <th>Close-Approach Date</th>\n",
       "      <th>Nominal Approch distance (au)</th>\n",
       "      <th>Min Close-Approach Distance (au)</th>\n",
       "      <th>Max Close-Approach Distance (au)</th>\n",
       "      <th>V Reletive (Km/s)</th>\n",
       "      <th>V Infinite (Km/s)</th>\n",
       "      <th>Close-Approach Uncertain Time</th>\n",
       "      <th>Absolute Magnitude (mag)</th>\n",
       "      <th>Diameter (Km)</th>\n",
       "      <th>Diameter-Sigma (Km)</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 BN7</td>\n",
       "      <td>6</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 02:16:00</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.088258</td>\n",
       "      <td>0.091431</td>\n",
       "      <td>5.258116</td>\n",
       "      <td>5.252461</td>\n",
       "      <td>1_13:09</td>\n",
       "      <td>23.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2020 BN7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017 MW4</td>\n",
       "      <td>18</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 02:18:00</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.061291</td>\n",
       "      <td>0.061310</td>\n",
       "      <td>17.591642</td>\n",
       "      <td>17.589171</td>\n",
       "      <td>00:01</td>\n",
       "      <td>20.050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2017 MW4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>509352</td>\n",
       "      <td>57</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 22:25:00</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.009639</td>\n",
       "      <td>8.686711</td>\n",
       "      <td>8.654807</td>\n",
       "      <td>00:02</td>\n",
       "      <td>20.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>509352 (2007 AG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214869</td>\n",
       "      <td>173</td>\n",
       "      <td>2.415027e+06</td>\n",
       "      <td>1900-01-07 10:39:00</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.050160</td>\n",
       "      <td>0.050177</td>\n",
       "      <td>11.594335</td>\n",
       "      <td>11.589753</td>\n",
       "      <td>00:02</td>\n",
       "      <td>16.510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214869 (2007 PA8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017 QD3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.415030e+06</td>\n",
       "      <td>1900-01-10 00:01:00</td>\n",
       "      <td>0.086560</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.086572</td>\n",
       "      <td>10.931727</td>\n",
       "      <td>10.928911</td>\n",
       "      <td>00:03</td>\n",
       "      <td>22.230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2017 QD3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46777</th>\n",
       "      <td>2022 YU3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 05:11:00</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.025017</td>\n",
       "      <td>0.025365</td>\n",
       "      <td>7.133786</td>\n",
       "      <td>7.118944</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YU3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46778</th>\n",
       "      <td>2022 YZ3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 09:29:00</td>\n",
       "      <td>0.060599</td>\n",
       "      <td>0.060118</td>\n",
       "      <td>0.061080</td>\n",
       "      <td>11.704659</td>\n",
       "      <td>11.700902</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YZ3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46779</th>\n",
       "      <td>2022 YY6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 11:07:00</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>20.269551</td>\n",
       "      <td>20.245368</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>26.109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YY6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46780</th>\n",
       "      <td>2022 YP5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 17:53:00</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>4.793711</td>\n",
       "      <td>4.765807</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>27.085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YP5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46781</th>\n",
       "      <td>2021 NF</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 21:51:00</td>\n",
       "      <td>0.045947</td>\n",
       "      <td>0.039555</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>11.326777</td>\n",
       "      <td>11.321656</td>\n",
       "      <td>03:09</td>\n",
       "      <td>24.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2021 NF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46782 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Designation Orbit Id  Time of Close approach  Close-Approach Date  \\\n",
       "0        2020 BN7        6            2.415024e+06  1900-01-04 02:16:00   \n",
       "1        2017 MW4       18            2.415024e+06  1900-01-04 02:18:00   \n",
       "2          509352       57            2.415024e+06  1900-01-04 22:25:00   \n",
       "3          214869      173            2.415027e+06  1900-01-07 10:39:00   \n",
       "4        2017 QD3        7            2.415030e+06  1900-01-10 00:01:00   \n",
       "...           ...      ...                     ...                  ...   \n",
       "46777    2022 YU3        5            2.459947e+06  2023-01-02 05:11:00   \n",
       "46778    2022 YZ3        3            2.459947e+06  2023-01-02 09:29:00   \n",
       "46779    2022 YY6        4            2.459947e+06  2023-01-02 11:07:00   \n",
       "46780    2022 YP5        6            2.459947e+06  2023-01-02 17:53:00   \n",
       "46781     2021 NF        6            2.459947e+06  2023-01-02 21:51:00   \n",
       "\n",
       "       Nominal Approch distance (au)  Min Close-Approach Distance (au)  \\\n",
       "0                           0.089661                          0.088258   \n",
       "1                           0.061300                          0.061291   \n",
       "2                           0.009632                          0.009625   \n",
       "3                           0.050169                          0.050160   \n",
       "4                           0.086560                          0.086548   \n",
       "...                              ...                               ...   \n",
       "46777                       0.025191                          0.025017   \n",
       "46778                       0.060599                          0.060118   \n",
       "46779                       0.005439                          0.005411   \n",
       "46780                       0.019977                          0.019919   \n",
       "46781                       0.045947                          0.039555   \n",
       "\n",
       "       Max Close-Approach Distance (au)  V Reletive (Km/s)  V Infinite (Km/s)  \\\n",
       "0                              0.091431           5.258116           5.252461   \n",
       "1                              0.061310          17.591642          17.589171   \n",
       "2                              0.009639           8.686711           8.654807   \n",
       "3                              0.050177          11.594335          11.589753   \n",
       "4                              0.086572          10.931727          10.928911   \n",
       "...                                 ...                ...                ...   \n",
       "46777                          0.025365           7.133786           7.118944   \n",
       "46778                          0.061080          11.704659          11.700902   \n",
       "46779                          0.005467          20.269551          20.245368   \n",
       "46780                          0.020036           4.793711           4.765807   \n",
       "46781                          0.052455          11.326777          11.321656   \n",
       "\n",
       "      Close-Approach Uncertain Time  Absolute Magnitude (mag)  Diameter (Km)  \\\n",
       "0                           1_13:09                    23.800            NaN   \n",
       "1                             00:01                    20.050            NaN   \n",
       "2                             00:02                    20.160            NaN   \n",
       "3                             00:02                    16.510            NaN   \n",
       "4                             00:03                    22.230            NaN   \n",
       "...                             ...                       ...            ...   \n",
       "46777                       < 00:01                    25.835            NaN   \n",
       "46778                       < 00:01                    25.088            NaN   \n",
       "46779                       < 00:01                    26.109            NaN   \n",
       "46780                       < 00:01                    27.085            NaN   \n",
       "46781                         03:09                    24.730            NaN   \n",
       "\n",
       "       Diameter-Sigma (Km)             Object  \n",
       "0                      NaN         (2020 BN7)  \n",
       "1                      NaN         (2017 MW4)  \n",
       "2                      NaN   509352 (2007 AG)  \n",
       "3                      NaN  214869 (2007 PA8)  \n",
       "4                      NaN         (2017 QD3)  \n",
       "...                    ...                ...  \n",
       "46777                  NaN         (2022 YU3)  \n",
       "46778                  NaN         (2022 YZ3)  \n",
       "46779                  NaN         (2022 YY6)  \n",
       "46780                  NaN         (2022 YP5)  \n",
       "46781                  NaN          (2021 NF)  \n",
       "\n",
       "[46782 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./raw_input_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf = ps.DataFrame(np.array(data['data']))\n",
    "# psdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Designation: string, Orbit Id: string, Time of Close approach: double, Close-Approach Date: timestamp, Nominal Approch distance (au): double, Min Close-Approach Distance (au): double, Max Close-Approach Distance (au): double, V Reletive (Km/s): double, V Infinite (Km/s): double, Close-Approach Uncertain Time: string, Absolute Magnitude (mag): double, Diameter (Km): double, Diameter-Sigma (Km): double, Object: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.read.csv('./raw_input_data.csv', inferSchema=True, header=True)\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|Designation|Orbit Id|Time of Close approach|Close-Approach Date|Nominal Approch distance (au)|Min Close-Approach Distance (au)|Max Close-Approach Distance (au)|V Reletive (Km/s)|V Infinite (Km/s)|Close-Approach Uncertain Time|Absolute Magnitude (mag)|Diameter (Km)|Diameter-Sigma (Km)|           Object|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|   2020 BN7|       6|     2415023.594589649|1900-01-04 02:16:00|           0.0896607474147164|              0.0882582365913522|              0.0914306781836958|  5.2581158476114| 5.25246109718832|                      1_13:09|                    23.8|         null|               null|       (2020 BN7)|\n",
      "|   2017 MW4|      18|     2415023.595882106|1900-01-04 02:18:00|           0.0613004997707699|              0.0612907488305461|              0.0613102521172838| 17.5916418448861| 17.5891708464552|                        00:01|                   20.05|         null|               null|       (2017 MW4)|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.fillna(value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|Designation|Orbit Id|Time of Close approach|Close-Approach Date|Nominal Approch distance (au)|Min Close-Approach Distance (au)|Max Close-Approach Distance (au)|V Reletive (Km/s)|V Infinite (Km/s)|Close-Approach Uncertain Time|Absolute Magnitude (mag)|Diameter (Km)|Diameter-Sigma (Km)|           Object|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|   2020 BN7|       6|     2415023.594589649|1900-01-04 02:16:00|           0.0896607474147164|              0.0882582365913522|              0.0914306781836958|  5.2581158476114| 5.25246109718832|                      1_13:09|                    23.8|         -1.0|               -1.0|       (2020 BN7)|\n",
      "|   2017 MW4|      18|     2415023.595882106|1900-01-04 02:18:00|           0.0613004997707699|              0.0612907488305461|              0.0613102521172838| 17.5916418448861| 17.5891708464552|                        00:01|                   20.05|         -1.0|               -1.0|       (2017 MW4)|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf = ps.DataFrame(\n",
    "#     {'a': [1, 2, 3, 4, 5, 6],\n",
    "#      'b': [100, 200, 300, 400, 500, 600],\n",
    "#      'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Designation</th>\n",
       "      <th>Orbit Id</th>\n",
       "      <th>Time of Close approach</th>\n",
       "      <th>Close-Approach Date</th>\n",
       "      <th>Nominal Approch distance (au)</th>\n",
       "      <th>Min Close-Approach Distance (au)</th>\n",
       "      <th>Max Close-Approach Distance (au)</th>\n",
       "      <th>V Reletive (Km/s)</th>\n",
       "      <th>V Infinite (Km/s)</th>\n",
       "      <th>Close-Approach Uncertain Time</th>\n",
       "      <th>Absolute Magnitude (mag)</th>\n",
       "      <th>Diameter (Km)</th>\n",
       "      <th>Diameter-Sigma (Km)</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 BN7</td>\n",
       "      <td>6</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 02:16:00</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.088258</td>\n",
       "      <td>0.091431</td>\n",
       "      <td>5.258116</td>\n",
       "      <td>5.252461</td>\n",
       "      <td>1_13:09</td>\n",
       "      <td>23.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2020 BN7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017 MW4</td>\n",
       "      <td>18</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 02:18:00</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.061291</td>\n",
       "      <td>0.061310</td>\n",
       "      <td>17.591642</td>\n",
       "      <td>17.589171</td>\n",
       "      <td>00:01</td>\n",
       "      <td>20.050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2017 MW4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>509352</td>\n",
       "      <td>57</td>\n",
       "      <td>2.415024e+06</td>\n",
       "      <td>1900-01-04 22:25:00</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.009639</td>\n",
       "      <td>8.686711</td>\n",
       "      <td>8.654807</td>\n",
       "      <td>00:02</td>\n",
       "      <td>20.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>509352 (2007 AG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214869</td>\n",
       "      <td>173</td>\n",
       "      <td>2.415027e+06</td>\n",
       "      <td>1900-01-07 10:39:00</td>\n",
       "      <td>0.050169</td>\n",
       "      <td>0.050160</td>\n",
       "      <td>0.050177</td>\n",
       "      <td>11.594335</td>\n",
       "      <td>11.589753</td>\n",
       "      <td>00:02</td>\n",
       "      <td>16.510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214869 (2007 PA8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017 QD3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.415030e+06</td>\n",
       "      <td>1900-01-10 00:01:00</td>\n",
       "      <td>0.086560</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.086572</td>\n",
       "      <td>10.931727</td>\n",
       "      <td>10.928911</td>\n",
       "      <td>00:03</td>\n",
       "      <td>22.230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2017 QD3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46777</th>\n",
       "      <td>2022 YU3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 05:11:00</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.025017</td>\n",
       "      <td>0.025365</td>\n",
       "      <td>7.133786</td>\n",
       "      <td>7.118944</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YU3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46778</th>\n",
       "      <td>2022 YZ3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 09:29:00</td>\n",
       "      <td>0.060599</td>\n",
       "      <td>0.060118</td>\n",
       "      <td>0.061080</td>\n",
       "      <td>11.704659</td>\n",
       "      <td>11.700902</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YZ3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46779</th>\n",
       "      <td>2022 YY6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 11:07:00</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>20.269551</td>\n",
       "      <td>20.245368</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>26.109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YY6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46780</th>\n",
       "      <td>2022 YP5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 17:53:00</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>4.793711</td>\n",
       "      <td>4.765807</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>27.085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YP5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46781</th>\n",
       "      <td>2021 NF</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 21:51:00</td>\n",
       "      <td>0.045947</td>\n",
       "      <td>0.039555</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>11.326777</td>\n",
       "      <td>11.321656</td>\n",
       "      <td>03:09</td>\n",
       "      <td>24.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2021 NF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46782 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Designation Orbit Id  Time of Close approach Close-Approach Date  \\\n",
       "0        2020 BN7        6            2.415024e+06 1900-01-04 02:16:00   \n",
       "1        2017 MW4       18            2.415024e+06 1900-01-04 02:18:00   \n",
       "2          509352       57            2.415024e+06 1900-01-04 22:25:00   \n",
       "3          214869      173            2.415027e+06 1900-01-07 10:39:00   \n",
       "4        2017 QD3        7            2.415030e+06 1900-01-10 00:01:00   \n",
       "...           ...      ...                     ...                 ...   \n",
       "46777    2022 YU3        5            2.459947e+06 2023-01-02 05:11:00   \n",
       "46778    2022 YZ3        3            2.459947e+06 2023-01-02 09:29:00   \n",
       "46779    2022 YY6        4            2.459947e+06 2023-01-02 11:07:00   \n",
       "46780    2022 YP5        6            2.459947e+06 2023-01-02 17:53:00   \n",
       "46781     2021 NF        6            2.459947e+06 2023-01-02 21:51:00   \n",
       "\n",
       "       Nominal Approch distance (au)  Min Close-Approach Distance (au)  \\\n",
       "0                           0.089661                          0.088258   \n",
       "1                           0.061300                          0.061291   \n",
       "2                           0.009632                          0.009625   \n",
       "3                           0.050169                          0.050160   \n",
       "4                           0.086560                          0.086548   \n",
       "...                              ...                               ...   \n",
       "46777                       0.025191                          0.025017   \n",
       "46778                       0.060599                          0.060118   \n",
       "46779                       0.005439                          0.005411   \n",
       "46780                       0.019977                          0.019919   \n",
       "46781                       0.045947                          0.039555   \n",
       "\n",
       "       Max Close-Approach Distance (au)  V Reletive (Km/s)  V Infinite (Km/s)  \\\n",
       "0                              0.091431           5.258116           5.252461   \n",
       "1                              0.061310          17.591642          17.589171   \n",
       "2                              0.009639           8.686711           8.654807   \n",
       "3                              0.050177          11.594335          11.589753   \n",
       "4                              0.086572          10.931727          10.928911   \n",
       "...                                 ...                ...                ...   \n",
       "46777                          0.025365           7.133786           7.118944   \n",
       "46778                          0.061080          11.704659          11.700902   \n",
       "46779                          0.005467          20.269551          20.245368   \n",
       "46780                          0.020036           4.793711           4.765807   \n",
       "46781                          0.052455          11.326777          11.321656   \n",
       "\n",
       "      Close-Approach Uncertain Time  Absolute Magnitude (mag)  Diameter (Km)  \\\n",
       "0                           1_13:09                    23.800            NaN   \n",
       "1                             00:01                    20.050            NaN   \n",
       "2                             00:02                    20.160            NaN   \n",
       "3                             00:02                    16.510            NaN   \n",
       "4                             00:03                    22.230            NaN   \n",
       "...                             ...                       ...            ...   \n",
       "46777                       < 00:01                    25.835            NaN   \n",
       "46778                       < 00:01                    25.088            NaN   \n",
       "46779                       < 00:01                    26.109            NaN   \n",
       "46780                       < 00:01                    27.085            NaN   \n",
       "46781                         03:09                    24.730            NaN   \n",
       "\n",
       "       Diameter-Sigma (Km)             Object  \n",
       "0                      NaN         (2020 BN7)  \n",
       "1                      NaN         (2017 MW4)  \n",
       "2                      NaN   509352 (2007 AG)  \n",
       "3                      NaN  214869 (2007 PA8)  \n",
       "4                      NaN         (2017 QD3)  \n",
       "...                    ...                ...  \n",
       "46777                  NaN         (2022 YU3)  \n",
       "46778                  NaN         (2022 YZ3)  \n",
       "46779                  NaN         (2022 YY6)  \n",
       "46780                  NaN         (2022 YP5)  \n",
       "46781                  NaN          (2021 NF)  \n",
       "\n",
       "[46782 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createSchema(columns):\n",
    "#     myScehma = []\n",
    "#     for key, value in dic.items():\n",
    "#         t = StructField(key, value, True)\n",
    "#         myScehma.append(t)\n",
    "#     # print(myScehma)\n",
    "#     return myScehma\n",
    "# scheme = createSchema(columns)\n",
    "# # print(type(scheme))\n",
    "# myScehma = StructType([i for i in scheme])\n",
    "# myScehma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data.json', 'w') as datafile:\n",
    "#     json.dump(data['data'], datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf = ps.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "c:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "mktime argument out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# sdf = spark.read.schema(myScehma).json('./data.json')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# sdf.printSchema()\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sdf \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(df)\n\u001b[0;32m      5\u001b[0m sdf\u001b[39m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    888\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[0;32m    894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[0;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    896\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    436\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[0;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:647\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mschema should be StructType or list or None, but got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m schema)\n\u001b[0;32m    646\u001b[0m \u001b[39m# convert python objects to sql data\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m internal_data \u001b[39m=\u001b[39m [struct\u001b[39m.\u001b[39mtoInternal(row) \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tupled_data]\n\u001b[0;32m    648\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39mparallelize(internal_data), struct\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:647\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mschema should be StructType or list or None, but got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m schema)\n\u001b[0;32m    646\u001b[0m \u001b[39m# convert python objects to sql data\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m internal_data \u001b[39m=\u001b[39m [struct\u001b[39m.\u001b[39;49mtoInternal(row) \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tupled_data]\n\u001b[0;32m    648\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39mparallelize(internal_data), struct\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:788\u001b[0m, in \u001b[0;36mStructType.toInternal\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\n\u001b[0;32m    784\u001b[0m         f\u001b[39m.\u001b[39mtoInternal(obj\u001b[39m.\u001b[39mget(n)) \u001b[39mif\u001b[39;00m c \u001b[39melse\u001b[39;00m obj\u001b[39m.\u001b[39mget(n)\n\u001b[0;32m    785\u001b[0m         \u001b[39mfor\u001b[39;00m n, f, c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfields, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needConversion)\n\u001b[0;32m    786\u001b[0m     )\n\u001b[0;32m    787\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m--> 788\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(\n\u001b[0;32m    789\u001b[0m         f\u001b[39m.\u001b[39;49mtoInternal(v) \u001b[39mif\u001b[39;49;00m c \u001b[39melse\u001b[39;49;00m v\n\u001b[0;32m    790\u001b[0m         \u001b[39mfor\u001b[39;49;00m f, v, c \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfields, obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_needConversion)\n\u001b[0;32m    791\u001b[0m     )\n\u001b[0;32m    792\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    793\u001b[0m     d \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:789\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\n\u001b[0;32m    784\u001b[0m         f\u001b[39m.\u001b[39mtoInternal(obj\u001b[39m.\u001b[39mget(n)) \u001b[39mif\u001b[39;00m c \u001b[39melse\u001b[39;00m obj\u001b[39m.\u001b[39mget(n)\n\u001b[0;32m    785\u001b[0m         \u001b[39mfor\u001b[39;00m n, f, c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfields, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needConversion)\n\u001b[0;32m    786\u001b[0m     )\n\u001b[0;32m    787\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m    788\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m--> 789\u001b[0m         f\u001b[39m.\u001b[39;49mtoInternal(v) \u001b[39mif\u001b[39;00m c \u001b[39melse\u001b[39;00m v\n\u001b[0;32m    790\u001b[0m         \u001b[39mfor\u001b[39;00m f, v, c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfields, obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needConversion)\n\u001b[0;32m    791\u001b[0m     )\n\u001b[0;32m    792\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    793\u001b[0m     d \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:591\u001b[0m, in \u001b[0;36mStructField.toInternal\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoInternal\u001b[39m(\u001b[39mself\u001b[39m, obj: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m--> 591\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataType\u001b[39m.\u001b[39;49mtoInternal(obj)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:216\u001b[0m, in \u001b[0;36mTimestampType.toInternal\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoInternal\u001b[39m(\u001b[39mself\u001b[39m, dt: datetime\u001b[39m.\u001b[39mdatetime) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m dt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m         seconds \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 216\u001b[0m             calendar\u001b[39m.\u001b[39mtimegm(dt\u001b[39m.\u001b[39mutctimetuple()) \u001b[39mif\u001b[39;00m dt\u001b[39m.\u001b[39mtzinfo \u001b[39melse\u001b[39;00m time\u001b[39m.\u001b[39;49mmktime(dt\u001b[39m.\u001b[39;49mtimetuple())\n\u001b[0;32m    217\u001b[0m         )\n\u001b[0;32m    218\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(seconds) \u001b[39m*\u001b[39m \u001b[39m1000000\u001b[39m \u001b[39m+\u001b[39m dt\u001b[39m.\u001b[39mmicrosecond\n",
      "\u001b[1;31mOverflowError\u001b[0m: mktime argument out of range"
     ]
    }
   ],
   "source": [
    "# sdf = spark.read.schema(myScehma).json('./data.json')\n",
    "# sdf.printSchema()\n",
    "\n",
    "sdf = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o46.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sdf\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o46.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "c:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Could not convert '6' with type str: tried to convert to int32\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "field Orbit Id: IntegerType() can not accept object '6' in type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m spark\u001b[39m.\u001b[39mconf\u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.execution.arrow.enabled\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m sdf \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(df, schema\u001b[39m=\u001b[39;49mmyScehma)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    888\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[0;32m    894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[0;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    896\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    436\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[0;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:628\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[1;32m--> 628\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(data)\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    631\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferSchemaFromList(data, names\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\session.py:910\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[39m@no_type_check\u001b[39m\n\u001b[0;32m    909\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(obj):\n\u001b[1;32m--> 910\u001b[0m     verify_func(obj)\n\u001b[0;32m    911\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:1722\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1720\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1721\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m verify_nullability(obj):\n\u001b[1;32m-> 1722\u001b[0m         verify_value(obj)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:1700\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1694\u001b[0m             new_msg(\n\u001b[0;32m   1695\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mLength of object (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) does not match with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1696\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlength of fields (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(obj), \u001b[39mlen\u001b[39m(verifiers))\n\u001b[0;32m   1697\u001b[0m             )\n\u001b[0;32m   1698\u001b[0m         )\n\u001b[0;32m   1699\u001b[0m     \u001b[39mfor\u001b[39;00m v, (_, verifier) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(obj, verifiers):\n\u001b[1;32m-> 1700\u001b[0m         verifier(v)\n\u001b[0;32m   1701\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1702\u001b[0m     d \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:1722\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1720\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1721\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m verify_nullability(obj):\n\u001b[1;32m-> 1722\u001b[0m         verify_value(obj)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:1635\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_integer\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify_integer\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1634\u001b[0m     assert_acceptable_types(obj)\n\u001b[1;32m-> 1635\u001b[0m     verify_acceptable_types(obj)\n\u001b[0;32m   1636\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2147483648\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39m>\u001b[39m \u001b[39m2147483647\u001b[39m:\n\u001b[0;32m   1637\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(new_msg(\u001b[39m\"\u001b[39m\u001b[39mobject of IntegerType out of range, got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m obj))\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\types.py:1592\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify_acceptable_types\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[39m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _acceptable_types[_type]:\n\u001b[1;32m-> 1592\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1593\u001b[0m             new_msg(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m can not accept object \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m in type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (dataType, obj, \u001b[39mtype\u001b[39m(obj)))\n\u001b[0;32m   1594\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: field Orbit Id: IntegerType() can not accept object '6' in type <class 'str'>"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "sdf = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Designation: string (nullable = true)\n",
      " |-- Orbit Id: string (nullable = true)\n",
      " |-- Time of Close approach: string (nullable = true)\n",
      " |-- Close-Approach Date: string (nullable = true)\n",
      " |-- Nominal Approch distance (au): string (nullable = true)\n",
      " |-- Min Close-Approach Distance (au): string (nullable = true)\n",
      " |-- Max Close-Approach Distance (au): string (nullable = true)\n",
      " |-- V Reletive (Km/s): string (nullable = true)\n",
      " |-- V Infinite (Km/s): string (nullable = true)\n",
      " |-- Close-Approach Uncertain Time: string (nullable = true)\n",
      " |-- Absolute Magnitude (mag): string (nullable = true)\n",
      " |-- Diameter (Km): string (nullable = true)\n",
      " |-- Diameter-Sigma (Km): string (nullable = true)\n",
      " |-- Object: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[1]\") \\\n",
    "#     .appName(\"SparkByExamples.com\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sparkDF=spark.createDataFrame(pandasDF) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()\n",
    "\n",
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "# Enable Apache Arrow to convert Pandas to PySpark DataFrame\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "sparkDF2=spark.createDataFrame(pandasDF) \n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o146.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sdf\u001b[39m.\u001b[39;49mshow(\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\dataengineer\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o146.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_distance_multiplier = 389.174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.25\n"
     ]
    }
   ],
   "source": [
    "num1 = 50\n",
    "num2 = 2\n",
    "num3 = 3\n",
    "num4 = 8\n",
    "result = num1/num4-num3*num2+num4\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current number is  1\n",
      "The current number is  2\n",
      "The current number is  3\n",
      "The current number is  4\n",
      "---------------------------\n",
      "The current number is  1\n",
      "The current number is  3\n",
      "The current number is  5\n",
      "---------------------------\n",
      "The current number is  5\n",
      "The current number is  4\n",
      "The current number is  3\n",
      "The current number is  2\n",
      "The current number is  1\n"
     ]
    }
   ],
   "source": [
    "for number in range(1,5):\n",
    "    print (\"The current number is \",number)\n",
    "print(\"---------------------------\")\n",
    "for number in range(1,7,2):\n",
    "    print (\"The current number is \",number)\n",
    "print(\"---------------------------\")\n",
    "for number in range(5,0,-1):\n",
    "    print (\"The current number is \",number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20 15 30 "
     ]
    }
   ],
   "source": [
    "for number in 10, 15:\n",
    "    for counter in range(1, 3):\n",
    "        print(number*counter, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 50 90 "
     ]
    }
   ],
   "source": [
    "for num in 23, 45, 50, 65, 76, 90:\n",
    "    if(num % 5 != 0):\n",
    "        continue\n",
    "    if(num % 10 == 0):\n",
    "        print(num, end=\" \")\n",
    "        continue\n",
    "    if(num % 3 == 0):\n",
    "        print(num, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "26\n",
      "27\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "number=28\n",
    "for num in range(25,30):\n",
    "    if(number>num):\n",
    "        print(num)\n",
    "    else:\n",
    "        print(num)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********effect of pass by value*********\n",
      "num_val before function call: 10\n",
      "num_val after function call: 10\n",
      "-----------------------------------------------\n",
      "*********effect of pass by reference*********\n",
      "val_list before function call: [5, 10, 15]\n",
      "val_list after function call: [5, 10, 15, 20]\n"
     ]
    }
   ],
   "source": [
    "def change_number(num):\n",
    "    num+=10\n",
    "def change_list(num_list):\n",
    "    num_list.append(20)\n",
    "num_val=10\n",
    "print(\"*********effect of pass by value*********\")\n",
    "print(\"num_val before function call:\", num_val)\n",
    "change_number(num_val)\n",
    "print(\"num_val after function call:\", num_val)\n",
    "print(\"-----------------------------------------------\")\n",
    "val_list=[5,10,15]\n",
    "print(\"*********effect of pass by reference*********\")\n",
    "print(\"val_list before function call:\", val_list)\n",
    "change_list(val_list)\n",
    "print(\"val_list after function call:\", val_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-1: positional arguments\n",
      "Flight Number: FN789\n",
      "Seating Capacity: 200\n",
      "-------------------------------------------------\n",
      "code-2: keyword arguments\n",
      "Flight Number: FN789\n",
      "Seating Capacity: 250\n",
      "-------------------------------------------------\n",
      "code-3: default arguments\n",
      "Flight Number: FN789\n",
      "Flight Make: Eagle\n",
      "Seating Capacity: 150\n",
      "-------------------------------------------------\n",
      "code-4: variable argument count\n",
      "Passenger name: Jack\n",
      "Total baggage weight in kg: 25\n"
     ]
    }
   ],
   "source": [
    "def display1(flight_number, seating_capacity):\n",
    "    print(\"Flight Number:\", flight_number)\n",
    "    print(\"Seating Capacity:\", seating_capacity)\n",
    "print(\"code-1: positional arguments\")\n",
    "display1(\"FN789\",200)\n",
    "#Uncomment and execute the below function call statement and observe the output\n",
    "#display1(300,\"FN123\")\n",
    "def display2(flight_number, seating_capacity):\n",
    "    print(\"Flight Number:\", flight_number)\n",
    "    print(\"Seating Capacity:\", seating_capacity)\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"code-2: keyword arguments\")\n",
    "display2(seating_capacity=250, flight_number=\"FN789\")\n",
    "def display3(flight_number, flight_make=\"Boeing\", seating_capacity=150):\n",
    "    print(\"Flight Number:\", flight_number)\n",
    "    print(\"Flight Make:\", flight_make)\n",
    "    print(\"Seating Capacity:\", seating_capacity)\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"code-3: default arguments\")\n",
    "display3(\"FN789\",\"Eagle\")\n",
    "#Uncomment and execute the below function call statements one by one and observe the output\n",
    "#display3(\"FN234\")\n",
    "#display3(\"FN678\",\"Qantas\",200)\n",
    "def display4(passenger_name, *baggage_tuple):\n",
    "    print(\"Passenger name:\",passenger_name)\n",
    "    total_wt=0\n",
    "    for baggage_wt in baggage_tuple:\n",
    "        total_wt+=baggage_wt\n",
    "    print(\"Total baggage weight in kg:\", total_wt)\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"code-4: variable argument count\")\n",
    "display4(\"Jack\",12,8,5)\n",
    "#Uncomment and execute the below function call statements one by one and observe the output\n",
    "#display4(\"Chan\",20,12)\n",
    "#display4(\"Henry\",23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come to Mys\n"
     ]
    }
   ],
   "source": [
    "message=\"welcome to Mysore\"\n",
    "word=message[-7:]\n",
    "if(word==\"Mysore\"):\n",
    "    print(\"got it\")\n",
    "else:\n",
    "    message=message[3:14]\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest integer greater than or equal to num1, 234.01 : 235\n",
      "The largest integer smaller than or equal to num1, 234.01 : 234\n",
      "The factorial of num2, 6 : 720\n",
      "The absolute value of num3 -27.01 : 27.01\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "num1=234.01\n",
    "num2=6\n",
    "num3=-27.01\n",
    "print(\"The smallest integer greater than or equal to num1,\",num1,\":\",math.ceil(num1))\n",
    "print(\"The largest integer smaller than or equal to num1,\",num1,\":\",math.floor(num1))\n",
    "print(\"The factorial of num2,\",num2,\":\", math.factorial(num2))\n",
    "print(\"The absolute value of num3\",num3,\":\",math.fabs(num3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update:\n",
      "Co-pilot: Raghav\n",
      "\n",
      "After update:\n",
      "Co-pilot: Henry\n",
      "Flight Attendant: Jane\n"
     ]
    }
   ],
   "source": [
    "crew_details={\n",
    "    \"Pilot\":\"Kumar\",\n",
    "    \"Co-pilot\":\"Raghav\",\n",
    "    \"Head-Strewardess\":\"Malini\",\n",
    "    \"Stewardess\":\"Mala\"\n",
    "}\n",
    "print(\"Before update:\")\n",
    "print(\"Co-pilot:\",crew_details.get(\"Co-pilot\"))\n",
    "crew_details.update({\"Flight Attendant\":\"Jane\", \"Co-pilot\":\"Henry\"})\n",
    "print(\"\\nAfter update:\")\n",
    "print(\"Co-pilot:\",crew_details.get(\"Co-pilot\"))\n",
    "print(\"Flight Attendant:\",crew_details[\"Flight Attendant\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [1, 2, 3, 4, 5, 6]\n",
    "A.pop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if(re.search(r\"Air\",\"Airline\")!=None):\n",
    "    print(\"Pattern found\")\n",
    "else:\n",
    "    print(\"Pattern not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "song=\"JINGLE Bells jingle Bells Jingle All The Way\"\n",
    "song.upper()\n",
    "song_words=song.split()\n",
    "count=0\n",
    "for word in song_words:\n",
    "    if(word.startswith(\"jingle\")):\n",
    "        count=count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5 10\n"
     ]
    }
   ],
   "source": [
    "sample_dict={'a':1,'b':2}\n",
    "sample_dict.update({'b':5, 'c':10 })\n",
    "print(sample_dict.get('a'),sample_dict.get('b'),sample_dict.get('c'))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New AirlineS4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word=\"New Airlines4\"\n",
    "if(re.search(r\"^N\",word) and re.search(r\"e$\",word)):\n",
    "    print(re.sub(r\"New\",r\"Old\",word))\n",
    "else:\n",
    "    print(re.sub(r\"s(\\d{1})\",r\"S\\1\",word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "num_list=[100.5,30.465,-1.22,20.15]\n",
    "num_list.insert(1, -100.5)\n",
    "num_list.pop(0)\n",
    "num_list.sort()\n",
    "print(math.ceil(math.fabs(num_list[0])))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "788d48d5a987e43a3acb566d0ceeb3b40aa1d059a2c93a8b85b6cecb5810c78f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
