{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Pakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import trim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://VAibhAv:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CNEO_data_extractor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e5c0db8280>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('CNEO_data_extractor').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.today().date()\n",
    "# date_min = date + timedelta(days=59)\n",
    "# date_max = date + timedelta(days=60)\n",
    "# print(date_min, date_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Previous/Old/Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|Designation|Orbit Id|Time of Close approach|Close-Approach Date|Nominal Approch distance (au)|Min Close-Approach Distance (au)|Max Close-Approach Distance (au)|V Reletive (Km/s)|V Infinite (Km/s)|Close-Approach Uncertain Time|Absolute Magnitude (mag)|Diameter (Km)|Diameter-Sigma (Km)|           Object|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|   2020 BN7|       6|     2415023.594589649|1900-01-04 02:16:00|           0.0896607474147164|              0.0882582365913522|              0.0914306781836958|  5.2581158476114| 5.25246109718832|                      1_13:09|                    23.8|         null|               null|       (2020 BN7)|\n",
      "|   2017 MW4|      18|     2415023.595882106|1900-01-04 02:18:00|           0.0613004997707699|              0.0612907488305461|              0.0613102521172838| 17.5916418448861| 17.5891708464552|                        00:01|                   20.05|         null|               null|       (2017 MW4)|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfl = spark.read.csv('./raw_input_data.csv', inferSchema=True, header=True)\n",
    "sdfl.show(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|Designation|Orbit Id|Time of Close approach|Close-Approach Date|Nominal Approch distance (au)|Min Close-Approach Distance (au)|Max Close-Approach Distance (au)|V Reletive (Km/s)|V Infinite (Km/s)|Close-Approach Uncertain Time|Absolute Magnitude (mag)|Diameter (Km)|Diameter-Sigma (Km)|           Object|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "|   2020 BN7|       6|     2415023.594589649|1900-01-04 02:16:00|           0.0896607474147164|              0.0882582365913522|              0.0914306781836958|  5.2581158476114| 5.25246109718832|                      1_13:09|                    23.8|         -1.0|               -1.0|       (2020 BN7)|\n",
      "|   2017 MW4|      18|     2415023.595882106|1900-01-04 02:18:00|           0.0613004997707699|              0.0612907488305461|              0.0613102521172838| 17.5916418448861| 17.5891708464552|                        00:01|                   20.05|         -1.0|               -1.0|       (2017 MW4)|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfl = sdfl.fillna(value=-1)\n",
    "sdfl.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfl = sdfl.withColumn('Object', trim('Object'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Last Record's Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-01-02'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = sdfl.tail(1)\n",
    "last_date = lr[0].asDict()['Close-Approach Date']\n",
    "# last_date = str(last_date.date() + timedelta(days=1))\n",
    "last_date = last_date.split(' ')[0]\n",
    "last_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+----------+\n",
      "|Designation|Orbit Id|Time of Close approach|Close-Approach Date|Nominal Approch distance (au)|Min Close-Approach Distance (au)|Max Close-Approach Distance (au)|V Reletive (Km/s)|V Infinite (Km/s)|Close-Approach Uncertain Time|Absolute Magnitude (mag)|Diameter (Km)|Diameter-Sigma (Km)|    Object|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+----------+\n",
      "|   2020 BN7|       6|     2415023.594589649|1900-01-04 02:16:00|           0.0896607474147164|              0.0882582365913522|              0.0914306781836958|  5.2581158476114| 5.25246109718832|                      1_13:09|                    23.8|         -1.0|               -1.0|(2020 BN7)|\n",
      "|   2017 MW4|      18|     2415023.595882106|1900-01-04 02:18:00|           0.0613004997707699|              0.0612907488305461|              0.0613102521172838| 17.5916418448861| 17.5891708464552|                        00:01|                   20.05|         -1.0|               -1.0|(2017 MW4)|\n",
      "+-----------+--------+----------------------+-------------------+-----------------------------+--------------------------------+--------------------------------+-----------------+-----------------+-----------------------------+------------------------+-------------+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfl.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = sdfl.toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Today's Data using API Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ssd-api.jpl.nasa.gov/cad.api\"\n",
    "parameters = {\n",
    "    \"date-min\": last_date,\n",
    "    \"date-max\": str(datetime.today().date()),\n",
    "    \"dist-max\": \"0.05\",\n",
    "    'fullname': \"true\",\n",
    "    'dist-max': \"0.1\",\n",
    "    'diameter': \"true\"\n",
    "}\n",
    "response = requests.get(url, parameters)\n",
    "data = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Designation': StringType,\n",
       " 'Orbit Id': StringType,\n",
       " 'Time of Close approach': DoubleType,\n",
       " 'Close-Approach Date': DateType,\n",
       " 'Nominal Approch distance (au)': DoubleType,\n",
       " 'Min Close-Approach Distance (au)': DoubleType,\n",
       " 'Max Close-Approach Distance (au)': DoubleType,\n",
       " 'V Reletive (Km/s)': DoubleType,\n",
       " 'V Infinite (Km/s)': DoubleType,\n",
       " 'Close-Approach Uncertain Time': StringType,\n",
       " 'Absolute Magnitude (mag)': DoubleType,\n",
       " 'Diameter (Km)': DoubleType,\n",
       " 'Diameter-Sigma (Km)': DoubleType,\n",
       " 'Object': StringType}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    'Designation',\n",
    "    'Orbit Id',\n",
    "    'Time of Close approach',\n",
    "    'Close-Approach Date',\n",
    "    'Nominal Approch distance (au)',\n",
    "    'Min Close-Approach Distance (au)',\n",
    "    'Max Close-Approach Distance (au)',\n",
    "    'V Reletive (Km/s)',\n",
    "    'V Infinite (Km/s)',\n",
    "    'Close-Approach Uncertain Time',\n",
    "    'Absolute Magnitude (mag)',\n",
    "    'Diameter (Km)',\n",
    "    'Diameter-Sigma (Km)',\n",
    "    'Object'\n",
    "]\n",
    "types = [\n",
    "    StringType(),\n",
    "    StringType(),\n",
    "    DoubleType(),\n",
    "    DateType(),\n",
    "    DoubleType(),\n",
    "    DoubleType(),\n",
    "    DoubleType(),\n",
    "    DoubleType(),\n",
    "    DoubleType(),\n",
    "    StringType(),\n",
    "    DoubleType(),\n",
    "    DoubleType(),\n",
    "    DoubleType(),\n",
    "    StringType(),\n",
    "         ]\n",
    "dic = {columns[i]: types[i] for i in range(len(columns))}\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Designation,StringType,true),StructField(Orbit Id,StringType,true),StructField(Time of Close approach,DoubleType,true),StructField(Close-Approach Date,DateType,true),StructField(Nominal Approch distance (au),DoubleType,true),StructField(Min Close-Approach Distance (au),DoubleType,true),StructField(Max Close-Approach Distance (au),DoubleType,true),StructField(V Reletive (Km/s),DoubleType,true),StructField(V Infinite (Km/s),DoubleType,true),StructField(Close-Approach Uncertain Time,StringType,true),StructField(Absolute Magnitude (mag),DoubleType,true),StructField(Diameter (Km),DoubleType,true),StructField(Diameter-Sigma (Km),DoubleType,true),StructField(Object,StringType,true)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def createSchema(columns):\n",
    "#     myScehma = []\n",
    "#     for key, value in dic.items():\n",
    "#         t = StructField(key, value, True)\n",
    "#         myScehma.append(t)\n",
    "#     # print(myScehma)\n",
    "#     return myScehma\n",
    "\n",
    "\n",
    "# scheme = createSchema(columns)\n",
    "# # print(type(scheme))\n",
    "# myScehma = StructType([i for i in scheme])\n",
    "# myScehma\n",
    "\n",
    "\n",
    "myScehma = StructType([\\\n",
    "    StructField('Designation', StringType(), True),\\\n",
    "    StructField('Orbit Id', StringType(), True),\\\n",
    "    StructField('Time of Close approach', DoubleType(), True),\\\n",
    "    StructField('Close-Approach Date', DateType(), True),\\\n",
    "    StructField('Nominal Approch distance (au)', DoubleType(), True),\\\n",
    "    StructField('Min Close-Approach Distance (au)', DoubleType(), True),\\\n",
    "    StructField('Max Close-Approach Distance (au)', DoubleType(), True),\\\n",
    "    StructField('V Reletive (Km/s)', DoubleType(), True),\\\n",
    "    StructField('V Infinite (Km/s)', DoubleType(), True),\\\n",
    "    StructField('Close-Approach Uncertain Time', StringType(), True),\\\n",
    "    StructField('Absolute Magnitude (mag)', DoubleType(), True),\\\n",
    "    StructField('Diameter (Km)', DoubleType(), True),\\\n",
    "    StructField('Diameter-Sigma (Km)', DoubleType(), True),\\\n",
    "    StructField('Object', StringType(), True)\\\n",
    "])\n",
    "# myScehma = StructType([\\\n",
    "#     StructField('Designation', StringType(), True),\\\n",
    "#     StructField('Orbit Id', StringType(), True),\\\n",
    "#     StructField('Time of Close approach', StringType(), True),\\\n",
    "#     StructField('Close-Approach Date', StringType(), True),\\\n",
    "#     StructField('Nominal Approch distance (au)', StringType(), True),\\\n",
    "#     StructField('Min Close-Approach Distance (au)', StringType(), True),\\\n",
    "#     StructField('Max Close-Approach Distance (au)', StringType(), True),\\\n",
    "#     StructField('V Reletive (Km/s)', StringType(), True),\\\n",
    "#     StructField('V Infinite (Km/s)', StringType(), True),\\\n",
    "#     StructField('Close-Approach Uncertain Time', StringType(), True),\\\n",
    "#     StructField('Absolute Magnitude (mag)', StringType(), True),\\\n",
    "#     StructField('Diameter (Km)', StringType(), True),\\\n",
    "#     StructField('Diameter-Sigma (Km)', StringType(), True),\\\n",
    "#     StructField('Object', StringType(), True)\\\n",
    "# ])\n",
    "myScehma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Designation': ['2022 YT3',\n",
       "  '2022 YU3',\n",
       "  '2022 YZ3',\n",
       "  '2022 YY6',\n",
       "  '2022 YP5',\n",
       "  '2021 NF',\n",
       "  '2022 YJ4',\n",
       "  '2015 AO43',\n",
       "  '2011 WR41',\n",
       "  '2019 AY3',\n",
       "  '2022 YS4',\n",
       "  '2022 XW1',\n",
       "  '2018 PN22'],\n",
       " 'Orbit Id': ['7',\n",
       "  '5',\n",
       "  '3',\n",
       "  '4',\n",
       "  '6',\n",
       "  '6',\n",
       "  '5',\n",
       "  '9',\n",
       "  '11',\n",
       "  '7',\n",
       "  '5',\n",
       "  '3',\n",
       "  '6'],\n",
       " 'Time of Close approach': ['2459946.529982869',\n",
       "  '2459946.716108034',\n",
       "  '2459946.895485196',\n",
       "  '2459946.962983696',\n",
       "  '2459947.244799544',\n",
       "  '2459947.410115729',\n",
       "  '2459947.741992184',\n",
       "  '2459947.934757015',\n",
       "  '2459948.221946792',\n",
       "  '2459949.029916917',\n",
       "  '2459949.348464181',\n",
       "  '2459949.356584550',\n",
       "  '2459949.488279088'],\n",
       " 'Close-Approach Date': ['2023-Jan-02 00:43',\n",
       "  '2023-Jan-02 05:11',\n",
       "  '2023-Jan-02 09:29',\n",
       "  '2023-Jan-02 11:07',\n",
       "  '2023-Jan-02 17:53',\n",
       "  '2023-Jan-02 21:51',\n",
       "  '2023-Jan-03 05:48',\n",
       "  '2023-Jan-03 10:26',\n",
       "  '2023-Jan-03 17:20',\n",
       "  '2023-Jan-04 12:43',\n",
       "  '2023-Jan-04 20:22',\n",
       "  '2023-Jan-04 20:33',\n",
       "  '2023-Jan-04 23:43'],\n",
       " 'Nominal Approch distance (au)': ['0.0284657568010055',\n",
       "  '0.0251910694591085',\n",
       "  '0.0605991524261425',\n",
       "  '0.00543886360452723',\n",
       "  '0.0199773082677155',\n",
       "  '0.045946866055314',\n",
       "  '0.0135605223870528',\n",
       "  '0.0826054551199632',\n",
       "  '0.0405377113458668',\n",
       "  '0.0429868823932621',\n",
       "  '0.0153711215662312',\n",
       "  '0.0910951372965543',\n",
       "  '0.0766612183849406'],\n",
       " 'Min Close-Approach Distance (au)': ['0.0283518611759164',\n",
       "  '0.0250168666339067',\n",
       "  '0.0601178894510354',\n",
       "  '0.00541107030146487',\n",
       "  '0.0199188916322011',\n",
       "  '0.0395550425857451',\n",
       "  '0.0135184677947036',\n",
       "  '0.0325049787467851',\n",
       "  '0.0405302257042255',\n",
       "  '0.0187140165092946',\n",
       "  '0.0152040621210323',\n",
       "  '0.0905449685597469',\n",
       "  '0.0764144828866867'],\n",
       " 'Max Close-Approach Distance (au)': ['0.0285796483222',\n",
       "  '0.0253652623678832',\n",
       "  '0.0610804106086901',\n",
       "  '0.00546665622803439',\n",
       "  '0.0200357211576115',\n",
       "  '0.0524548848376105',\n",
       "  '0.0136025737320697',\n",
       "  '0.135505833277371',\n",
       "  '0.111676045470241',\n",
       "  '0.0780981747389915',\n",
       "  '0.0155381537857632',\n",
       "  '0.0916453275375157',\n",
       "  '0.0769079613009998'],\n",
       " 'V Reletive (Km/s)': ['6.43976919501424',\n",
       "  '7.13378641737377',\n",
       "  '11.7046594149789',\n",
       "  '20.2695510903181',\n",
       "  '4.79371109446374',\n",
       "  '11.3267768155877',\n",
       "  '5.57880952862473',\n",
       "  '8.46115194400779',\n",
       "  '8.94882792980961',\n",
       "  '19.7428515460153',\n",
       "  '6.79731772382462',\n",
       "  '6.04222271584813',\n",
       "  '3.78252394414714'],\n",
       " 'V Infinite (Km/s)': ['6.42521761094422',\n",
       "  '7.11894423823365',\n",
       "  '11.7009022805467',\n",
       "  '20.245367587082',\n",
       "  '4.76580690667688',\n",
       "  '11.3216558918964',\n",
       "  '5.54347723391237',\n",
       "  '8.45733889836805',\n",
       "  '8.94147999472359',\n",
       "  '19.7397117526823',\n",
       "  '6.77176800006143',\n",
       "  '6.03737993842712',\n",
       "  '3.7733240374348'],\n",
       " 'Close-Approach Uncertain Time': ['< 00:01',\n",
       "  '< 00:01',\n",
       "  '< 00:01',\n",
       "  '< 00:01',\n",
       "  '< 00:01',\n",
       "  '03:09',\n",
       "  '< 00:01',\n",
       "  '9_07:59',\n",
       "  '6_22:43',\n",
       "  '1_14:23',\n",
       "  '< 00:01',\n",
       "  '00:07',\n",
       "  '02:23'],\n",
       " 'Absolute Magnitude (mag)': ['25.814',\n",
       "  '25.835',\n",
       "  '25.088',\n",
       "  '26.109',\n",
       "  '27.085',\n",
       "  '24.73',\n",
       "  '26.916',\n",
       "  '26.5',\n",
       "  '25.12',\n",
       "  '23.8',\n",
       "  '25.583',\n",
       "  '25.175',\n",
       "  '27.5'],\n",
       " 'Diameter (Km)': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'Diameter-Sigma (Km)': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'Object': ['       (2022 YT3)',\n",
       "  '       (2022 YU3)',\n",
       "  '       (2022 YZ3)',\n",
       "  '       (2022 YY6)',\n",
       "  '       (2022 YP5)',\n",
       "  '       (2021 NF)',\n",
       "  '       (2022 YJ4)',\n",
       "  '       (2015 AO43)',\n",
       "  '       (2011 WR41)',\n",
       "  '       (2019 AY3)',\n",
       "  '       (2022 YS4)',\n",
       "  '       (2022 XW1)',\n",
       "  '       (2018 PN22)']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = data['data']\n",
    "modified_raw_data = {}\n",
    "for i in range(len(columns)):\n",
    "    for j in range(len(t)):\n",
    "        if columns[i] in modified_raw_data:\n",
    "            modified_raw_data[columns[i]].append(t[j][i])\n",
    "        else:\n",
    "            modified_raw_data[columns[i]] = [t[j][i]]\n",
    "modified_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>des</th>\n",
       "      <th>orbit_id</th>\n",
       "      <th>jd</th>\n",
       "      <th>cd</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_min</th>\n",
       "      <th>dist_max</th>\n",
       "      <th>v_rel</th>\n",
       "      <th>v_inf</th>\n",
       "      <th>t_sigma_f</th>\n",
       "      <th>h</th>\n",
       "      <th>diameter</th>\n",
       "      <th>diameter_sigma</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022 YT3</td>\n",
       "      <td>7</td>\n",
       "      <td>2459946.529982869</td>\n",
       "      <td>2023-Jan-02 00:43</td>\n",
       "      <td>0.0284657568010055</td>\n",
       "      <td>0.0283518611759164</td>\n",
       "      <td>0.0285796483222</td>\n",
       "      <td>6.43976919501424</td>\n",
       "      <td>6.42521761094422</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.814</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2022 YT3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022 YU3</td>\n",
       "      <td>5</td>\n",
       "      <td>2459946.716108034</td>\n",
       "      <td>2023-Jan-02 05:11</td>\n",
       "      <td>0.0251910694591085</td>\n",
       "      <td>0.0250168666339067</td>\n",
       "      <td>0.0253652623678832</td>\n",
       "      <td>7.13378641737377</td>\n",
       "      <td>7.11894423823365</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.835</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2022 YU3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022 YZ3</td>\n",
       "      <td>3</td>\n",
       "      <td>2459946.895485196</td>\n",
       "      <td>2023-Jan-02 09:29</td>\n",
       "      <td>0.0605991524261425</td>\n",
       "      <td>0.0601178894510354</td>\n",
       "      <td>0.0610804106086901</td>\n",
       "      <td>11.7046594149789</td>\n",
       "      <td>11.7009022805467</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.088</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2022 YZ3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022 YY6</td>\n",
       "      <td>4</td>\n",
       "      <td>2459946.962983696</td>\n",
       "      <td>2023-Jan-02 11:07</td>\n",
       "      <td>0.00543886360452723</td>\n",
       "      <td>0.00541107030146487</td>\n",
       "      <td>0.00546665622803439</td>\n",
       "      <td>20.2695510903181</td>\n",
       "      <td>20.245367587082</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>26.109</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2022 YY6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022 YP5</td>\n",
       "      <td>6</td>\n",
       "      <td>2459947.244799544</td>\n",
       "      <td>2023-Jan-02 17:53</td>\n",
       "      <td>0.0199773082677155</td>\n",
       "      <td>0.0199188916322011</td>\n",
       "      <td>0.0200357211576115</td>\n",
       "      <td>4.79371109446374</td>\n",
       "      <td>4.76580690667688</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>27.085</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(2022 YP5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        des orbit_id                 jd                 cd  \\\n",
       "0  2022 YT3        7  2459946.529982869  2023-Jan-02 00:43   \n",
       "1  2022 YU3        5  2459946.716108034  2023-Jan-02 05:11   \n",
       "2  2022 YZ3        3  2459946.895485196  2023-Jan-02 09:29   \n",
       "3  2022 YY6        4  2459946.962983696  2023-Jan-02 11:07   \n",
       "4  2022 YP5        6  2459947.244799544  2023-Jan-02 17:53   \n",
       "\n",
       "                  dist             dist_min             dist_max  \\\n",
       "0   0.0284657568010055   0.0283518611759164      0.0285796483222   \n",
       "1   0.0251910694591085   0.0250168666339067   0.0253652623678832   \n",
       "2   0.0605991524261425   0.0601178894510354   0.0610804106086901   \n",
       "3  0.00543886360452723  0.00541107030146487  0.00546665622803439   \n",
       "4   0.0199773082677155   0.0199188916322011   0.0200357211576115   \n",
       "\n",
       "              v_rel             v_inf t_sigma_f       h diameter  \\\n",
       "0  6.43976919501424  6.42521761094422   < 00:01  25.814     None   \n",
       "1  7.13378641737377  7.11894423823365   < 00:01  25.835     None   \n",
       "2  11.7046594149789  11.7009022805467   < 00:01  25.088     None   \n",
       "3  20.2695510903181   20.245367587082   < 00:01  26.109     None   \n",
       "4  4.79371109446374  4.76580690667688   < 00:01  27.085     None   \n",
       "\n",
       "  diameter_sigma           fullname  \n",
       "0           None         (2022 YT3)  \n",
       "1           None         (2022 YU3)  \n",
       "2           None         (2022 YZ3)  \n",
       "3           None         (2022 YY6)  \n",
       "4           None         (2022 YP5)  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data['data'], columns=data['fields'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>des</th>\n",
       "      <th>orbit_id</th>\n",
       "      <th>jd</th>\n",
       "      <th>cd</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_min</th>\n",
       "      <th>dist_max</th>\n",
       "      <th>v_rel</th>\n",
       "      <th>v_inf</th>\n",
       "      <th>t_sigma_f</th>\n",
       "      <th>h</th>\n",
       "      <th>diameter</th>\n",
       "      <th>diameter_sigma</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011 WR41</td>\n",
       "      <td>11</td>\n",
       "      <td>2.459948e+06</td>\n",
       "      <td>2023-01-03 17:20:00</td>\n",
       "      <td>0.040538</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.111676</td>\n",
       "      <td>8.948828</td>\n",
       "      <td>8.941480</td>\n",
       "      <td>6_22:43</td>\n",
       "      <td>25.120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2011 WR41)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019 AY3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.459949e+06</td>\n",
       "      <td>2023-01-04 12:43:00</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.078098</td>\n",
       "      <td>19.742852</td>\n",
       "      <td>19.739712</td>\n",
       "      <td>1_14:23</td>\n",
       "      <td>23.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2019 AY3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022 YS4</td>\n",
       "      <td>5</td>\n",
       "      <td>2.459949e+06</td>\n",
       "      <td>2023-01-04 20:22:00</td>\n",
       "      <td>0.015371</td>\n",
       "      <td>0.015204</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>6.797318</td>\n",
       "      <td>6.771768</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YS4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022 XW1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.459949e+06</td>\n",
       "      <td>2023-01-04 20:33:00</td>\n",
       "      <td>0.091095</td>\n",
       "      <td>0.090545</td>\n",
       "      <td>0.091645</td>\n",
       "      <td>6.042223</td>\n",
       "      <td>6.037380</td>\n",
       "      <td>00:07</td>\n",
       "      <td>25.175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 XW1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018 PN22</td>\n",
       "      <td>6</td>\n",
       "      <td>2.459949e+06</td>\n",
       "      <td>2023-01-04 23:43:00</td>\n",
       "      <td>0.076661</td>\n",
       "      <td>0.076414</td>\n",
       "      <td>0.076908</td>\n",
       "      <td>3.782524</td>\n",
       "      <td>3.773324</td>\n",
       "      <td>02:23</td>\n",
       "      <td>27.500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2018 PN22)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          des orbit_id            jd                  cd      dist  dist_min  \\\n",
       "8   2011 WR41       11  2.459948e+06 2023-01-03 17:20:00  0.040538  0.040530   \n",
       "9    2019 AY3        7  2.459949e+06 2023-01-04 12:43:00  0.042987  0.018714   \n",
       "10   2022 YS4        5  2.459949e+06 2023-01-04 20:22:00  0.015371  0.015204   \n",
       "11   2022 XW1        3  2.459949e+06 2023-01-04 20:33:00  0.091095  0.090545   \n",
       "12  2018 PN22        6  2.459949e+06 2023-01-04 23:43:00  0.076661  0.076414   \n",
       "\n",
       "    dist_max      v_rel      v_inf t_sigma_f       h  diameter  \\\n",
       "8   0.111676   8.948828   8.941480   6_22:43  25.120       NaN   \n",
       "9   0.078098  19.742852  19.739712   1_14:23  23.800       NaN   \n",
       "10  0.015538   6.797318   6.771768   < 00:01  25.583       NaN   \n",
       "11  0.091645   6.042223   6.037380     00:07  25.175       NaN   \n",
       "12  0.076908   3.782524   3.773324     02:23  27.500       NaN   \n",
       "\n",
       "    diameter_sigma            fullname  \n",
       "8              NaN         (2011 WR41)  \n",
       "9              NaN          (2019 AY3)  \n",
       "10             NaN          (2022 YS4)  \n",
       "11             NaN          (2022 XW1)  \n",
       "12             NaN         (2018 PN22)  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['jd'] = pd.to_numeric(df['jd'])\n",
    "df['cd'] = pd.to_datetime(df['cd'])\n",
    "df['dist'] = pd.to_numeric(df['dist'])\n",
    "df['dist_min'] = pd.to_numeric(df['dist_min'])\n",
    "df['dist_max'] = pd.to_numeric(df['dist_max'])\n",
    "df['v_rel'] = pd.to_numeric(df['v_rel'])\n",
    "df['v_inf'] = pd.to_numeric(df['v_inf'])\n",
    "df['t_sigma_f'] = df['t_sigma_f'].astype(str)\n",
    "df['h'] = pd.to_numeric(df['h'])\n",
    "df['diameter'] = pd.to_numeric(df['diameter'])\n",
    "df['diameter_sigma'] = pd.to_numeric(df['diameter_sigma'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Designation</th>\n",
       "      <th>Orbit Id</th>\n",
       "      <th>Time of Close approach</th>\n",
       "      <th>Close-Approach Date</th>\n",
       "      <th>Nominal Approch distance (au)</th>\n",
       "      <th>Min Close-Approach Distance (au)</th>\n",
       "      <th>Max Close-Approach Distance (au)</th>\n",
       "      <th>V Reletive (Km/s)</th>\n",
       "      <th>V Infinite (Km/s)</th>\n",
       "      <th>Close-Approach Uncertain Time</th>\n",
       "      <th>Absolute Magnitude (mag)</th>\n",
       "      <th>Diameter (Km)</th>\n",
       "      <th>Diameter-Sigma (Km)</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022 YT3</td>\n",
       "      <td>7</td>\n",
       "      <td>2.459947e+06</td>\n",
       "      <td>2023-01-02 00:43:00</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>0.028352</td>\n",
       "      <td>0.02858</td>\n",
       "      <td>6.439769</td>\n",
       "      <td>6.425218</td>\n",
       "      <td>&lt; 00:01</td>\n",
       "      <td>25.814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2022 YT3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Designation Orbit Id  Time of Close approach Close-Approach Date  \\\n",
       "0    2022 YT3        7            2.459947e+06 2023-01-02 00:43:00   \n",
       "\n",
       "   Nominal Approch distance (au)  Min Close-Approach Distance (au)  \\\n",
       "0                       0.028466                          0.028352   \n",
       "\n",
       "   Max Close-Approach Distance (au)  V Reletive (Km/s)  V Infinite (Km/s)  \\\n",
       "0                           0.02858           6.439769           6.425218   \n",
       "\n",
       "  Close-Approach Uncertain Time  Absolute Magnitude (mag)  Diameter (Km)  \\\n",
       "0                       < 00:01                    25.814            NaN   \n",
       "\n",
       "   Diameter-Sigma (Km)             Object  \n",
       "0                  NaN         (2022 YT3)  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = columns\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Designation: string, Orbit Id: string, Time of Close approach: double, Close-Approach Date: date, Nominal Approch distance (au): double, Min Close-Approach Distance (au): double, Max Close-Approach Distance (au): double, V Reletive (Km/s): double, V Infinite (Km/s): double, Close-Approach Uncertain Time: string, Absolute Magnitude (mag): double, Diameter (Km): double, Diameter-Sigma (Km): double, Object: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsdf = spark.createDataFrame(df, schema = myScehma)\n",
    "nsdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o116.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 13) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:492)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nsdf\u001b[39m.\u001b[39;49mshow(\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o116.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 13) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:492)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "nsdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2022 YJ4',\n",
       "  '5',\n",
       "  '2459947.741992184',\n",
       "  '2023-Jan-03 05:48',\n",
       "  '0.0135605223870528',\n",
       "  '0.0135184677947036',\n",
       "  '0.0136025737320697',\n",
       "  '5.57880952862473',\n",
       "  '5.54347723391237',\n",
       "  '< 00:01',\n",
       "  '26.916',\n",
       "  None,\n",
       "  None,\n",
       "  '       (2022 YJ4)'),\n",
       " ('2015 AO43',\n",
       "  '9',\n",
       "  '2459947.934757015',\n",
       "  '2023-Jan-03 10:26',\n",
       "  '0.0826054551199632',\n",
       "  '0.0325049787467851',\n",
       "  '0.135505833277371',\n",
       "  '8.46115194400779',\n",
       "  '8.45733889836805',\n",
       "  '9_07:59',\n",
       "  '26.5',\n",
       "  None,\n",
       "  None,\n",
       "  '       (2015 AO43)'),\n",
       " ('2011 WR41',\n",
       "  '11',\n",
       "  '2459948.221946792',\n",
       "  '2023-Jan-03 17:20',\n",
       "  '0.0405377113458668',\n",
       "  '0.0405302257042255',\n",
       "  '0.111676045470241',\n",
       "  '8.94882792980961',\n",
       "  '8.94147999472359',\n",
       "  '6_22:43',\n",
       "  '25.12',\n",
       "  None,\n",
       "  None,\n",
       "  '       (2011 WR41)')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [tuple(d) for d in data['data']]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 9) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nsdfrdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39;49mtoDF(columns)\n\u001b[0;32m      2\u001b[0m nsdfrdd\u001b[39m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m@no_type_check\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoDF\u001b[39m(\u001b[39mself\u001b[39m, schema\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sampleRatio\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     76\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m sparkSession\u001b[39m.\u001b[39;49mcreateDataFrame(\u001b[39mself\u001b[39;49m, schema, sampleRatio)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[1;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[0;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    896\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m    933\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, RDD):\n\u001b[1;32m--> 934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromRDD(data\u001b[39m.\u001b[39;49mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 600\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchema(rdd, samplingRatio, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    601\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    602\u001b[0m     tupled_rdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inferSchema\u001b[39m(\n\u001b[0;32m    526\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    527\u001b[0m     rdd: RDD[Any],\n\u001b[0;32m    528\u001b[0m     samplingRatio: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    529\u001b[0m     names: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    530\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructType:\n\u001b[0;32m    531\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[39m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     first \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39;49mfirst()\n\u001b[0;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m first:\n\u001b[0;32m    548\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe first row in RDD is empty, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcan not infer schema\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m   1891\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1903\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   1904\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[0;32m   1905\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   1885\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   1487\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\anaconda3\\envs\\azure\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 9) (VAibhAv executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "nsdfrdd = rdd.toDF(columns)\n",
    "nsdfrdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsdf = spark.createDataFrame(modified_raw_data, schema=myScehma)\n",
    "# nsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array(data['data']), columns = data['fields'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jd'] = pd.to_numeric(df['jd'])\n",
    "df['cd'] = pd.to_datetime(df['cd'])\n",
    "df['dist'] = pd.to_numeric(df['dist'])\n",
    "df['dist_min'] = pd.to_numeric(df['dist_min'])\n",
    "df['dist_max'] = pd.to_numeric(df['dist_max'])\n",
    "df['v_rel'] = pd.to_numeric(df['v_rel'])\n",
    "df['v_inf'] = pd.to_numeric(df['v_inf'])\n",
    "df['h'] = pd.to_numeric(df['h'])\n",
    "df['diameter'] = pd.to_numeric(df['diameter'])\n",
    "df['diameter_sigma'] = pd.to_numeric(df['diameter_sigma'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./raw_input_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./raw_input_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf = ps.DataFrame(np.array(data['data']))\n",
    "# psdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv('./raw_input_data.csv', inferSchema=True, header=True)\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.fillna(value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf = ps.DataFrame(\n",
    "#     {'a': [1, 2, 3, 4, 5, 6],\n",
    "#      'b': [100, 200, 300, 400, 500, 600],\n",
    "#      'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data.json', 'w') as datafile:\n",
    "#     json.dump(data['data'], datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psdf = ps.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf = spark.read.schema(myScehma).json('./data.json')\n",
    "# sdf.printSchema()\n",
    "\n",
    "sdf = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "sdf = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[1]\") \\\n",
    "#     .appName(\"SparkByExamples.com\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sparkDF=spark.createDataFrame(pandasDF) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()\n",
    "\n",
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "# Enable Apache Arrow to convert Pandas to PySpark DataFrame\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "sparkDF2=spark.createDataFrame(pandasDF) \n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_distance_multiplier = 389.174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "788d48d5a987e43a3acb566d0ceeb3b40aa1d059a2c93a8b85b6cecb5810c78f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
